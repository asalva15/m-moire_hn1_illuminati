{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfjqiN7+j77FmVgCh3HDEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asalva15/memoire_hn1_illuminati_GIT/blob/main/Collocations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ahegel/collocations/blob/master/get_collocations27.py"
      ],
      "metadata": {
        "id": "XXwkF0jVsH-c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQeOR1NGr7dE"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python2.7 version originale du code\n",
        "\n",
        "\"\"\"\n",
        "This program finds collocations in a corpus of text. It can find both the\n",
        "overall top collocations, and the collocations of keywords you enter manually.\n",
        "\"\"\"\n",
        "\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# find collocations for each word\n",
        "def get_collocations(corpus, windowsize=10, numresults=10):\n",
        "    '''This function uses the Natural Language Toolkit to find the top collocations in a corpus.\n",
        "    It takes as an argument a string that contains the corpus you want to\n",
        "    find collocations from. It prints the top collocations it finds.\n",
        "    '''\n",
        "    # convert the corpus (a string) into  a list of words\n",
        "    tokens = word_tokenize(corpus)\n",
        "    # initialize the bigram association measures object to score each collocation\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    # initialize the bigram collocation finder object to find and rank collocations\n",
        "    finder = BigramCollocationFinder.from_words(tokens, window_size=windowsize)\n",
        "    # apply a series of filters to narrow down the collocation results\n",
        "    ignored_words = stopwords.words('english')\n",
        "    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
        "    finder.apply_freq_filter(1)\n",
        "    # calculate the top results by T-score\n",
        "    # list of all possible measures: .raw_freq, .pmi, .likelihood_ratio, .chi_sq, .phi_sq, .fisher, .student_t, .mi_like, .poisson_stirling, .jaccard, .dice\n",
        "    results = finder.nbest(bigram_measures.student_t, numresults)\n",
        "    # print the results\n",
        "    print \"Top \" + str(numresults) + \" collocations:\"\n",
        "    for k, v in results:\n",
        "        print str(k) + \", \" + str(v)\n",
        "\n",
        "\n",
        "def get_keyword_collocations(corpus, keyword, windowsize=10, numresults=10):\n",
        "    '''This function uses the Natural Language Toolkit to find collocations\n",
        "    for a specific keyword in a corpus. It takes as an argument a string that\n",
        "    contains the corpus you want to find collocations from. It prints the top\n",
        "    collocations it finds for each keyword.\n",
        "    '''\n",
        "    # convert the corpus (a string) into  a list of words\n",
        "    tokens = word_tokenize(corpus)\n",
        "    # initialize the bigram association measures object to score each collocation\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    # initialize the bigram collocation finder object to find and rank collocations\n",
        "    finder = BigramCollocationFinder.from_words(tokens, window_size=windowsize)\n",
        "    # initialize a function that will narrow down collocates that don't contain the keyword\n",
        "    keyword_filter = lambda *w: keyword not in w\n",
        "    # apply a series of filters to narrow down the collocation results\n",
        "    ignored_words = stopwords.words('english')\n",
        "    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
        "    finder.apply_freq_filter(1)\n",
        "    finder.apply_ngram_filter(keyword_filter)\n",
        "    # calculate the top results by T-score\n",
        "    # list of all possible measures: .raw_freq, .pmi, .likelihood_ratio, .chi_sq, .phi_sq, .fisher, .student_t, .mi_like, .poisson_stirling, .jaccard, .dice\n",
        "    results = finder.nbest(bigram_measures.student_t, numresults)\n",
        "    # print the results\n",
        "    print \"Top collocations for \" + str(keyword) + \":\"\n",
        "    collocations = ''\n",
        "    for k, v in results:\n",
        "        if k != keyword:\n",
        "            collocations += k + ' '\n",
        "        else:\n",
        "            collocations += v + ' '\n",
        "    print collocations + '\\n'\n",
        "\n",
        "\n",
        "# Replace this with your filename\n",
        "infile = \"sample_corpus.txt\"\n",
        "\n",
        "# Read in the corpus you want to find collocations from\n",
        "with open(infile) as tmpfile:\n",
        "    data = tmpfile.read()\n",
        "\n",
        "# Clean the data\n",
        "data = data.translate(None, string.punctuation)  # remove punctuation\n",
        "data = \"\".join(i for i in data if ord(i) < 128)  # remove non-ascii characters\n",
        "\n",
        "# Get the top collocations for the entire corpus\n",
        "get_collocations(data)\n",
        "print ' '\n",
        "\n",
        "# Replace this with a list of keywords you want to find collocations for\n",
        "words_of_interest = [\"love\", \"death\"]\n",
        "\n",
        "# Get the top collocations for each keyword in the list above\n",
        "for word in words_of_interest:\n",
        "    get_keyword_collocations(data, word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "corpus = '/content/drive/My Drive/MemoireHN1/corpus17761850'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6voP8aVruSSA",
        "outputId": "9c3f2b7c-34e3-4359-a0f1-685679aee2ac"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Function to read contents of a directory and concatenate text files into one string\n",
        "def concatenate_text_files(directory):\n",
        "    # Initialize an empty string to store the concatenated text\n",
        "    concatenated_text = \"\"\n",
        "\n",
        "    # Iterate over each file in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        # Check if the file is a text file\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Read the contents of the text file and append it to the concatenated text\n",
        "            with open(os.path.join(directory, filename), 'r') as file:\n",
        "                concatenated_text += file.read()\n",
        "\n",
        "    # Return the concatenated text\n",
        "    return concatenated_text\n",
        "\n",
        "# Specify the directory containing the text files\n",
        "directory_path = \"/content/drive/My Drive/MemoireHN1/corpus17761850\"\n",
        "\n",
        "# Call the function to concatenate text files\n",
        "concatenated_text = concatenate_text_files(directory_path)\n",
        "\n",
        "# Specify the path for the new concatenated text file\n",
        "output_file_path = \"/content/drive/My Drive/MemoireHN1/concatenated_text.txt\"\n",
        "\n",
        "# Write the concatenated text to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    output_file.write(concatenated_text)\n",
        "\n",
        "print(\"Concatenated text saved to:\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk52TPbp4xQC",
        "outputId": "9866e287-e901-4cc9-a4cc-7ac95319bd60"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated text saved to: /content/drive/My Drive/MemoireHN1/concatenated_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_0Xajqp4R7N",
        "outputId": "0f6cbe17-e70b-48a8-c27a-a7382391075e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Top 10 collocations:\n",
            "United, States\n",
            "New, York\n",
            "tho, tho\n",
            "one, hundred\n",
            "Mr, Mr\n",
            "per, cent\n",
            "hundred, dollars\n",
            "00, 00\n",
            "one, one\n",
            "per, per\n",
            " \n",
            "Top collocations for Illuminati:\n",
            "Top 10 collocations:\n",
            "Top collocations for French:\n",
            "Top 10 collocations:\n",
            "Top collocations for Bavarian:\n",
            "Top 10 collocations:\n",
            "Top collocations for France:\n",
            "Top 10 collocations:\n",
            "Top collocations for Weishaupt:\n",
            "Top 10 collocations:\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python2.7\n",
        "\n",
        "\"\"\"\n",
        "This program finds collocations in a corpus of text. It can find both the\n",
        "overall top collocations, and the collocations of keywords you enter manually.\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# find collocations for each word\n",
        "def get_collocations(corpus, windowsize=10, numresults=10):\n",
        "    '''This function uses the Natural Language Toolkit to find the top collocations in a corpus.\n",
        "    It takes as an argument a string that contains the corpus you want to\n",
        "    find collocations from. It prints the top collocations it finds.\n",
        "    '''\n",
        "    # convert the corpus (a string) into  a list of words\n",
        "    tokens = word_tokenize(corpus)\n",
        "    # initialize the bigram association measures object to score each collocation\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    # initialize the bigram collocation finder object to find and rank collocations\n",
        "    finder = BigramCollocationFinder.from_words(tokens, window_size=windowsize)\n",
        "    # apply a series of filters to narrow down the collocation results\n",
        "    ignored_words = stopwords.words('english')\n",
        "    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
        "    finder.apply_freq_filter(1)\n",
        "    # calculate the top results by T-score\n",
        "    # list of all possible measures: .raw_freq, .pmi, .likelihood_ratio, .chi_sq, .phi_sq, .fisher, .student_t, .mi_like, .poisson_stirling, .jaccard, .dice\n",
        "    results = finder.nbest(bigram_measures.student_t, numresults)\n",
        "    # print the results\n",
        "    print(\"Top \" + str(numresults) + \" collocations:\")\n",
        "    for k, v in results:\n",
        "        print(str(k) + \", \" + str(v))\n",
        "\n",
        "\n",
        "\n",
        "def get_keyword_collocations(corpus, keyword, windowsize=10, numresults=10):\n",
        "    '''This function uses the Natural Language Toolkit to find collocations\n",
        "    for a specific keyword in a corpus. It takes as an argument a string that\n",
        "    contains the corpus you want to find collocations from. It prints the top\n",
        "    collocations it finds for each keyword.\n",
        "    '''\n",
        "    # convert the corpus (a string) into  a list of words\n",
        "    tokens = word_tokenize(corpus)\n",
        "    # initialize the bigram association measures object to score each collocation\n",
        "    bigram_measures = BigramAssocMeasures()\n",
        "    # initialize the bigram collocation finder object to find and rank collocations\n",
        "    finder = BigramCollocationFinder.from_words(tokens, window_size=windowsize)\n",
        "    # initialize a function that will narrow down collocates that don't contain the keyword\n",
        "    keyword_filter = lambda *w: keyword not in w\n",
        "    # apply a series of filters to narrow down the collocation results\n",
        "    ignored_words = stopwords.words('english')\n",
        "    finder.apply_word_filter(lambda w: len(w) < 2 or w.lower() in ignored_words)\n",
        "    finder.apply_freq_filter(1)\n",
        "    finder.apply_ngram_filter(keyword_filter)\n",
        "    # calculate the top results by T-score\n",
        "    # list of all possible measures: .raw_freq, .pmi, .likelihood_ratio, .chi_sq, .phi_sq, .fisher, .student_t, .mi_like, .poisson_stirling, .jaccard, .dice\n",
        "    results = finder.nbest(bigram_measures.student_t, numresults)\n",
        "    # print the results\n",
        "    print(\"Top collocations for \" + str(keyword) + \":\")\n",
        "    collocations = ''\n",
        "    for k, v in results:\n",
        "        if k != keyword:\n",
        "            collocations += k + ' '\n",
        "        else:\n",
        "            collocations += v + ' '\n",
        "    print(\"Top \" + str(numresults) + \" collocations:\")\n",
        "\n",
        "\n",
        "# Replace this with your filename\n",
        "infile = \"/content/drive/My Drive/MemoireHN1/concatenated_text.txt\"\n",
        "\n",
        "# Read in the corpus you want to find collocations from\n",
        "with open(infile) as tmpfile:\n",
        "    data = tmpfile.read()\n",
        "\n",
        "# Clean the data\n",
        "# Create a translation table to remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "# Clean the data by applying the translation table\n",
        "data = data.translate(translator)\n",
        "data = \"\".join(i for i in data if ord(i) < 128)  # remove non-ascii characters\n",
        "\n",
        "# Get the top collocations for the entire corpus\n",
        "get_collocations(data)\n",
        "print(' ')\n",
        "\n",
        "# Replace this with a list of keywords you want to find collocations for\n",
        "words_of_interest = [\"Illuminati\", \"French\", \"Bavarian\",\"France\",\"Weishaupt\"]\n",
        "\n",
        "# Get the top collocations for each keyword in the list above\n",
        "for word in words_of_interest:\n",
        "    get_keyword_collocations(data, word)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ItnVSH-r_0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tentative alternative avec code suivant : https://github.com/jamesosullivan/collocates/blob/main/co-occurrences.py"
      ],
      "metadata": {
        "id": "3Y23jz9E9Dxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure necessary resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def co_occurrence(text, window_size=2):\n",
        "    # Load stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text and filter out stopwords\n",
        "    tokens = [token.lower() for token in word_tokenize(text) if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "    # Initialize co-occurrence count\n",
        "    co_occurrence_counts = Counter()\n",
        "\n",
        "    # Calculate co-occurrences within the specified window size\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                co_occurred_token = tokens[j]\n",
        "                co_occurrence_counts[(token, co_occurred_token)] += 1\n",
        "\n",
        "    # Return the top 10 most common co-occurrences\n",
        "    return co_occurrence_counts.most_common(10)\n",
        "\n",
        "# Read the text file\n",
        "with open(\"filename.txt\", \"r\", encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Calculate the top 10 co-occurrences\n",
        "top_co_occurrences = co_occurrence(text, window_size=2)\n",
        "\n",
        "# Output the results\n",
        "with open(\"top-co-occurrence-results.txt\", \"w\", encoding='utf-8') as output_file:\n",
        "    for pair, freq in top_co_occurrences:\n",
        "        output_file.write(f\"{pair[0]}, {pair[1]}: {freq}\\n\")\n",
        "\n",
        "print(\"Top 10 co-occurrence analysis is complete. Results are saved in 'top-co-occurrence-results.txt'.\")"
      ],
      "metadata": {
        "id": "zM30Q_ZDr_2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure necessary resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def co_occurrence(text, window_size=2):\n",
        "    # Load stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Tokenize the text and filter out stopwords\n",
        "    tokens = [token.lower() for token in word_tokenize(text) if token.isalpha() and token.lower() not in stop_words]\n",
        "\n",
        "    # Initialize co-occurrence count\n",
        "    co_occurrence_counts = Counter()\n",
        "\n",
        "    # Calculate co-occurrences within the specified window size\n",
        "    for i in range(len(tokens)):\n",
        "        token = tokens[i]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(tokens), i + window_size + 1)\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                co_occurred_token = tokens[j]\n",
        "                co_occurrence_counts[(token, co_occurred_token)] += 1\n",
        "\n",
        "    # Return the top 10 most common co-occurrences\n",
        "    return co_occurrence_counts.most_common(10)\n",
        "\n",
        "# Read the text file\n",
        "with open(\"/content/drive/My Drive/MemoireHN1/concatenated_text.txt\", \"r\", encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Calculate the top 10 co-occurrences\n",
        "top_co_occurrences = co_occurrence(text, window_size=2)\n",
        "\n",
        "# Output the results\n",
        "with open(\"top-co-occurrence-results.txt\", \"w\", encoding='utf-8') as output_file:\n",
        "    for pair, freq in top_co_occurrences:\n",
        "        output_file.write(f\"{pair[0]}, {pair[1]}: {freq}\\n\")\n",
        "print(top_co_occurrences)\n",
        "print(\"Top 10 co-occurrence analysis is complete. Results are saved in 'top-co-occurrence-results.txt'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J3jrKE-r_4u",
        "outputId": "bb2c75c3-5c0d-41a8-f9ad-17626b54d6cc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('united', 'states'), 372), (('states', 'united'), 372), (('new', 'york'), 253), (('york', 'new'), 253), (('r', 'r'), 220), (('f', 'r'), 199), (('r', 'f'), 199), (('j', 'j'), 184), (('r', 'j'), 161), (('j', 'r'), 161)]\n",
            "Top 10 co-occurrence analysis is complete. Results are saved in 'top-co-occurrence-results.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sAOcfa_r_6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmJsavACr_9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HntKZKSr__O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}